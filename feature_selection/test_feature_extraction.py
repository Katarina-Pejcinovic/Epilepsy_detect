# -*- coding: utf-8 -*-
"""test_feature_extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1APpQeTv64DoQb7R_KWlVL2ewx1M8Xv4t
"""

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

def test_feature_extraction():
    # Function to generate a signal with added noise
    def generate_signal(amplitude, frequency):
        sample_rate = 1000
        duration = 1
        t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)
        signal = amplitude * np.sin(2 * np.pi * frequency * t)

        # Add a small amount of random noise to the signal
        noise = np.random.normal(0, 1, len(t))  # Adjust noise amplitude as needed
        signal_with_noise = signal + noise
        return signal_with_noise

    # Function to generate signals for clusters
    def generate_cluster_signals(cluster, num_signals):
        if cluster == 1:
            return [generate_signal(5, 5) for _ in range(num_signals)]
        elif cluster == 2:
            return [generate_signal(3, 33) for _ in range(num_signals)]
        else:
            return None

    # Generate signals for both clusters
    signals_cluster1 = generate_cluster_signals(1, 10)
    signals_cluster2 = generate_cluster_signals(2, 10)

    # Combine signals from both clusters
    all_signals = signals_cluster1 + signals_cluster2

    all_features = get_features(all_signals)

    # Apply PCA for dimensionality reduction
    pca = PCA(n_components=2)
    principal_components = pca.fit_transform(all_features)

    # Apply K-means clustering
    kmeans = KMeans(n_clusters=2, random_state=42, n_init='auto')
    pred_labels = kmeans.fit_predict(all_features)

    # Create true labels based on known clusters
    true_labels = np.array([0] * 10 + [1] * 10)  # 0 for cluster 1, 1 for cluster 2

    # Calculate accuracy
    accuracy = accuracy_score(true_labels, pred_labels)

    # Plot two sample signals from each cluster on separate horizontal subplots
    fig, axs = plt.subplots(1, 2, figsize=(12, 6))

    for i, signals in enumerate([signals_cluster1, signals_cluster2]):
        for j in range(2):
            axs[i].plot(signals[j], label=f'Signal {j+1}', alpha=0.7 if j == 0 else 1.0)
            axs[i].set_title(f'Cluster {i+1} Sample Signals')
            axs[i].set_xlabel('Time')
            axs[i].set_ylabel('Amplitude')
            axs[i].legend()

    plt.tight_layout()
    plt.show()

    # Plot principal components (features) with circles around clusters
    plt.figure(figsize=(8, 6))
    plt.scatter(principal_components[:, 0], principal_components[:, 1], c=pred_labels, cmap='viridis', marker='o')

    plt.title('PCA of Features with K-means Clustering')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.colorbar(label='Cluster')
    plt.show()

    return f"Accuracy of K-means clustering of 20 signals (5 vs. 33 Hz) based on our 21 extracted features: {accuracy * 100:.2f}%"