# -*- coding: utf-8 -*-
"""classical_ml_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f4ZuWpch2j-WfEY6MyBLpcoF_o5AvqSS
"""

# Use for validation

def svm_model(data, labels, val_data, svm_param):

  # For validation, train on full 3/4 data and then test on untouched 1/4 validation data?

  # PARAMETERS: kernal, C, gamma, degree

  # # Read CSV
  # data = pd.read_csv('your_csv_file.csv')

  # # Prepare the data
  # X = data.drop('target_column_name', axis=1)  # Replace 'target_column_name' with the actual name of your target column
  # y = data['target_column_name']  # Replace 'target_column_name' with the actual name of your target column

  # # Split the data into training and testing sets
  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  # # Scale the data
  # scaler = StandardScaler()
  # X_train = scaler.fit_transform(X_train)
  # X_test = scaler.transform(X_test)

  # # Create SVM model
  # svm_model = SVC(kernel='k', random_state=42)  # You can choose different kernels depending on your problem

  # # Train the model
  # svm_model.fit(X_train, y_train)

  # # Make predictions
  # y_pred = svm_model.predict(X_test)
  X_train = data
  y_train = labels
  X_test = val_data

  # Pipeline w/ UMAP + SVM

  # Create UMAP object
  reducer = umap.UMAP(n_components=[], n_neighbors=[], min_dist=[], metrics=[])

  # Turn data into z-scores
  scl = StandardScaler()
  X_train = scl.fit_transform(X_train)
  X_test = scl.fit_transform(X_test)

  # Data has been reduced into two features from four
  X_train = reducer.fit_transform(X_train)
  X_test = reducer.fit_transform(X_test)

  # Train the model
  svm_model = SVC(kernel=[], C=[], gamma=[], degree=[], random_state=42)
  svc_pipeline.fit(X_train, y_train)

  # Make predictions
  y_pred = svc_pipeline.predict(X_test)

  return y_pred

  # Calculate the accuracy
  # accuracy = accuracy_score(y_test, y_pred)
  # print(f"SVM Accuracy: {accuracy}")

def random_forest_model(data, labels, val_data, rf_param):

  # PARAMETERS: n_estimators, min_samples_leaf, max_features

  # Pipeline w/ UMAP + RF

  # Create UMAP object
  reducer = umap.UMAP(n_components=[], n_neighbors=[], min_dist=[], metrics=[])

  # Turn data into z-scores
  scl = StandardScaler()
  X_train = scl.fit_transform(X_train)
  X_test = scl.fit_transform(X_test)

  # Data has been reduced into two features from four
  X_train = reducer.fit_transform(X_train)
  X_test = reducer.fit_transform(X_test)


  y_pred = []
  return y_pred

  print(f"Random Forest Accuracy: ")

def xg_boost_model(data, labels, xg_param):

  # PARAMETERS: n_components = 2, _covariance_type

  # Pipeline w/ UMAP + XG Boost

  # Create UMAP object
  reducer = umap.UMAP(n_components=[], n_neighbors=[], min_dist=[], metrics=[])

  # Turn data into z-scores
  scl = StandardScaler()
  X_train = scl.fit_transform(X_train)
  X_test = scl.fit_transform(X_test)

  # Data has been reduced into two features from four
  X_train = reducer.fit_transform(X_train)
  X_test = reducer.fit_transform(X_test)

  y_pred = []
  return y_pred

  print(f"XG Boost Accuracy: ")

def kmeans_model(data, labels, kmeans_param):

  # PARAMETERS: n_clusters

  # Pipeline w/ UMAP + K Means

  # Create UMAP object
  reducer = umap.UMAP(n_components=[], n_neighbors=[], min_dist=[], metrics=[])

  # Turn data into z-scores
  scl = StandardScaler()
  X_train = scl.fit_transform(X_train)
  X_test = scl.fit_transform(X_test)

  # Data has been reduced into two features from four
  X_train = reducer.fit_transform(X_train)
  X_test = reducer.fit_transform(X_test)

  y_pred = []
  return y_pred

  print(f"K-Means Accuracy: ")