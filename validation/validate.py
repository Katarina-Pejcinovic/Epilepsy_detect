# -*- coding: utf-8 -*-
"""validate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S05I44rIGCgPM5rcir8GxhLO54FeXEbf


"""

from classical_ML.classical_ml_models import *
from deep_learning.rnn import *
from deep_learning.cnn import *

def validate(train_data, train_labels, validation_data, validation_labels, train_data_ml, train_labels_ml, 
             validation_data_ml, validation_labels_ml, parameters):

  import numpy as np
  from sklearn.metrics import fbeta_score
  from sklearn.metrics import confusion_matrix

  y_true = validation_labels

  ## Run models

  svm_pred = svm_model(train_data_ml, train_labels_ml, validation_data_ml, parameters[0])
  svm_temp = svm_pred[1]
  pat1 = svm_temp[0:16,:]
  pat2 = svm_temp[16:32,:]
  pat1_pred = np.mean(pat1, axis=0)
  pat2_pred = np.mean(pat2, axis=0)
  pat_pred = np.append(pat1_pred, pat2_pred, axis=0)
  svm_pred = pat_pred.reshape(2,2)

  rf_pred = random_forest_model(train_data_ml, train_labels_ml, validation_data_ml, parameters[1])
  rf_temp = rf_pred[1]
  pat1 = rf_temp[0:16,:]
  pat2 = rf_temp[16:32,:]
  pat1_pred = np.mean(pat1, axis=0)
  pat2_pred = np.mean(pat2, axis=0)
  pat_pred = np.append(pat1_pred, pat2_pred, axis=0)
  rf_pred = pat_pred.reshape(2,2)

  xg_pred = xg_boost_model(train_data_ml, train_labels_ml, validation_data_ml, parameters[3])
  xg_temp = xg_pred[1]
  pat1 = xg_temp[0:16,:]
  pat2 = xg_temp[16:32,:]
  pat1_pred = np.mean(pat1, axis=0)
  pat2_pred = np.mean(pat2, axis=0)
  pat_pred = np.append(pat1_pred, pat2_pred, axis=0)
  xg_pred = pat_pred.reshape(2,2)

  gmm_pred = gmm_model(train_data_ml, train_labels_ml, validation_data_ml, parameters[4])
  gmm_temp = gmm_pred[1]
  pat1 = gmm_temp[0:16,:]
  pat2 = gmm_temp[16:32,:]
  pat1_pred = np.mean(pat1, axis=0)
  pat2_pred = np.mean(pat2, axis=0)
  pat_pred = np.append(pat1_pred, pat2_pred, axis=0)
  gmm_pred = pat_pred.reshape(2,2)


#run cnn model and obtain the model instance, predictions on test datset (1, 0), and probabilities (decimals)
  model_cnn, cnn_pred, cnn_proba = run_CNN(train_data, train_labels, validation_data, validation_labels)

  # RNN
  rnn_pred = rnn_model(train_data, train_labels, validation_data, epochs=3)


  # Compare using F2 scoring (beta > 1 gives more weight to recall)
  svm_f2_score = fbeta_score(y_true, svm_pred[0], average='weighted', beta=2)
  rf_f2_score = fbeta_score(y_true, rf_pred[0], average='weighted', beta=2)
  xg_f2_score = fbeta_score(y_true, xg_pred[0], average='weighted', beta=2)
  gmm_f2_score = fbeta_score(y_true, gmm_pred[0], average='weighted', beta=2)
  cnn_f2_score = fbeta_score(y_true, cnn_pred, average='weighted', beta=2)
  rnn_f2_score = fbeta_score(y_true, rnn_pred[0], average='weighted', beta=2)

  # Compare using confusion matrices
  svm_cm = confusion_matrix(y_true, svm_pred[0])
  rf_cm = confusion_matrix(y_true, rf_pred[0])
  xg_cm = confusion_matrix(y_true, xg_pred[0])
  gmm_cm = confusion_matrix(y_true, gmm_pred[0])
  cnn_cm = confusion_matrix(y_true, cnn_pred)
  rnn_cm = confusion_matrix(y_true, rnn_pred[0])

  # F2 Highest Score
  results_f2_score = [svm_f2_score, rf_f2_score, xg_f2_score, gmm_f2_score, cnn_f2_score, rnn_f2_score]
  #results_f2_score = [svm_f2_score, rf_f2_score, xg_f2_score, gmm_f2_score, rnn_f2_score]
  print("The model with the highest f2 score is", max(results_f2_score, key=lambda x: x))
  with open('figure_list.txt', 'a') as f:
     f.write(f"The model with the highest f2 score is {max(results_f2_score, key=lambda x: x)}")

  # Compare using ROC curves
  # model_names = ['SVM', 'Random Forest', 'HMM', 'KMeans', 'CNN', 'RNN']
  model_names = ['SVM', 'Random Forest', 'XG Boost', 'Gaussian Mixture', 'CNN','RNN']

  # for i, pred in enumerate([svm_pred, rf_pred, hmm_pred, kmeans_pred, cnn_pred, rnn_pred]):
  for i, pred in enumerate([svm_pred[1], rf_pred[1], xg_pred[1], gmm_pred[1], cnn_pred[1],rnn_pred[1]]):
    fpr, tpr, _ = roc_curve(y_true, pred)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Receiver Operating Characteristic (ROC) Curve for {model_names[i]}')
    plt.legend(loc="lower right")
    plt.show()
    plt.savefig("{}_roc_auc.jpg".format(model_names[i]))
    with open('figure_list.txt', 'a') as f:
        f.write('{}_roc_auc.jpg\n'.format(model_names[i]))

  # Confusion matrices
  # confusion_matrices = [svm_cm,rf_cm,hmm_cm,kmeans_cm,cnn_cm,rnn_cm]
  confusion_matrices = [svm_cm, rf_cm, xg_cm, gmm_cm,cnn_cm, rnn_cm]

  for i, matrix in enumerate(confusion_matrices):
    true_positives = matrix[1][1]
    false_positives = matrix[0][1]
    false_negatives = matrix[1][0]
    true_negatives = matrix[0][0]

    # Calculate precision, accuracy, and recall
    precision = true_positives / (true_positives + false_positives)
    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + false_negatives + true_negatives)
    recall = true_positives / (true_positives + false_negatives)

    # Print the results
    print(matrix)
    print(f"Precision: {precision:.2f}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Recall: {recall:.2f}")

    with open('figure_list.txt', 'a') as f:
        f.write(model_names[i])
        f.write(str(matrix))
        f.write(f'Precision: {precision}')
        f.write(f'Precision: {accuracy}')
        f.write(f'Precision: {recall}')

