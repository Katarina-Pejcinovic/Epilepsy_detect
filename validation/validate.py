# -*- coding: utf-8 -*-
"""validate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S05I44rIGCgPM5rcir8GxhLO54FeXEbf
"""

def validate(train_data, train_labels, validation_data, validation_labels, parameters):

  import numpy as np
  from sklearn.metrics import fbeta_score
  from sklearn.metrics import confusion_matrix

  ## Run models

  svm_pred = svm_model(train_data, train_labels, validation_data, parameters)

  rf_pred = random_forest_model(train_data, train_labels, validation_data, parameters)

  hmm_pred = hmm_model(train_data, train_labels, validation_data, parameters)

  kmeans_pred = kmeans_model(train_data, train_labels, validation_data, parameters)

  # CNN

  # RNN
  rnn_pred = rnn_model(train_data, train_labels, validation_data)


  # Compare using F2 scoring (beta > 1 gives more weight to recall)
  svm_f2_score = fbeta_score(y_true, svm_pred, average='weighted', beta=2)
  rf_f2_score = fbeta_score(y_true, rf_pred, average='weighted', beta=2)
  hmm_f2_score = fbeta_score(y_true, hmm_pred, average='weighted', beta=2)
  kmeans_f2_score = fbeta_score(y_true, kmeans_pred, average='weighted', beta=2)
  cnn_f2_score = fbeta_score(y_true, cnn_pred, average='weighted', beta=2)
  rnn_f2_score = fbeta_score(y_true, rnn_pred, average='weighted', beta=2)

  # Compare using confusion matrices
  svm_cm = confusion_matrix(y_true, svm_pred)
  rf_cm = confusion_matrix(y_true, rf_pred)
  hmm_cm = confusion_matrix(y_true, hmm_pred)
  kmeans_cm = confusion_matrix(y_true, kmeans_pred)
  cnn_cm = confusion_matrix(y_true, cnn_pred)
  rnn_cm = confusion_matrix(y_true, rnn_pred)

  # F2 Highest Score
  results_f2_score = [svm_f2_score, rf_f2_score, hmm_f2_score, kmeans_f2_score, cnn_f2_score, rnn_f2_score]
  print("The model with the highest f2 score is", max(results_f2_score, key=lambda x: x))


  # Compare using ROC curves
  for i in [svm_pred, rf_pred, hmm_pred, kmeans_pred, cnn_pred, rnn_pred]:
    fpr, tpr, _ = roc_curve(y_true, i)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve for', i)
    plt.legend(loc="lower right")
    plt.show()

  # Confusion matrices
  confusion_matrices = [svm_cm,rf_cm,hmm_cm,kmeans_cm,cnn_cm,rnn_cm]

  for matrix in confusion_matrices:
    true_positives = matrix[1][1]
    false_positives = matrix[0][1]
    false_negatives = matrix[1][0]
    true_negatives = matrix[0][0]

    # Calculate precision, accuracy, and recall
    precision = true_positives / (true_positives + false_positives)
    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + false_negatives + true_negatives)
    recall = true_positives / (true_positives + false_negatives)

    # Print the results
    print(matrix)
    print(f"Precision: {precision:.2f}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Recall: {recall:.2f}")

